import datetime
import pickle
import traceback
import hashlib

import util
from environment import Environment
from q_function import InverseLength, RandomQFunction

import torch
import wandb
from tqdm import tqdm


class SuccessRatePolicyEvaluator:
    """Evaluates the policy derived from a Q function by its success rate at solving
       problems generated by an environment."""
    def __init__(self, environment, config):
        self.environment = environment
        self.seed = config.get('seed', 0)
        self.n_problems = config.get('n_problems', 100)  # How many problems to use.
        self.max_steps = config.get('max_steps', 30)  # Maximum length of an episode.
        self.beam_size = config.get('beam_size', 1)  # Size of the beam in beam search.
        self.debug = config.get('debug', False)  # Whether to print all steps during evaluation.

    def evaluate(self, q, verbose=False, show_progress=False):
        successes, failures, solution_lengths = [], [], []
        wrapper = tqdm if show_progress else lambda x: x

        for i in wrapper(range(self.n_problems)):
            problem = self.environment.generate_new(seed=(self.seed + i))
            success, history = q.rollout(self.environment, problem,
                                         self.max_steps, self.beam_size, self.debug)
            if success:
                successes.append((i, problem))
            else:
                failures.append((i, problem))
            solution_lengths.append(len(history) - 1 if success else -1)
            if verbose:
                print(i, problem, '-- success?', success)

        return {
            'success_rate': len(successes) / self.n_problems,
            'solution_lengths': solution_lengths,
            'max_solution_length': max(solution_lengths),
            'successes': successes,
            'failures': failures,
        }


class EndOfLearning(Exception):
    '''Exception used to signal the end of the learning budget for an agent.'''


class EnvironmentWithEvaluationProxy:
    '''Wrapper around the environment that triggers an evaluation every K calls'''
    def __init__(self, experiment_id, agent, environment, config={}):
        self.experiment_id = experiment_id
        self.environment = environment
        self.n_steps = 0

        self.evaluate_every = config['evaluate_every']
        self.eval_config = config['eval_config']
        self.agent = agent
        self.output_path = config['output']
        self.checkpoint_path = config['checkpoint_path']
        self.max_steps = config['max_steps']
        self.print_every = config.get('print_every', 100)

        self.results = []
        self.n_new_problems = 0
        self.cumulative_reward = 0
        self.begin_time = datetime.datetime.now()
        self.n_checkpoints = 0

    def generate_new(self, domain=None, seed=None):
        self.n_new_problems += 1
        return self.environment.generate_new(domain, seed)

    def step(self, states, domain=None):
        n_steps_before = self.n_steps
        self.n_steps += len(states)

        # If the number of steps crossed the boundary of '0 mod evaluate_every', run evaluation.
        # If the agent took one step at a time, then we would only need to test if
        # n_steps % evaluate_every == 0. However the agent might take multiple steps at once.
        if (n_steps_before % self.evaluate_every) + len(states) >= self.evaluate_every:
            self.evaluate()

        if self.n_steps >= self.max_steps:
            # Budget ended.
            raise EndOfLearning()

        reward_and_actions = self.environment.step(states, domain)
        self.cumulative_reward += sum(rw for rw, _ in reward_and_actions)

        # Same logic as with evaluate_every.
        if (n_steps_before % self.print_every) + len(states) >= self.print_every:
            self.print_progress()

        return reward_and_actions

    def evaluate(self):
        print('Evaluating...')
        name, domain = self.agent.name(), self.environment.default_domain

        evaluator = SuccessRatePolicyEvaluator(self.environment, self.eval_config)
        results = evaluator.evaluate(self.agent.get_q_function())
        results['n_steps'] = self.n_steps
        results['name'] = name
        results['domain'] = domain
        results['problems_seen'] = self.n_new_problems
        results['cumulative_reward'] = self.cumulative_reward

        wandb.log({'success_rate': results['success_rate'],
                   'problems_seen': results['problems_seen'],
                   'n_environment_steps': results['n_steps'],
                   'cumulative_reward': results['cumulative_reward'],
                   'max_solution_length': results['max_solution_length'],
                   })

        print('Success rate:', results['success_rate'],
              '\tMax length:', results['max_solution_length'])

        output_path = self.output_path.format(self.experiment_id)

        try:
            with open(output_path, 'rb') as f:
                existing_results = pickle.load(f)
        except Exception as e:
            print(f'Starting new results log at {self.output_path} ({e})')
            existing_results = []

        existing_results.append(results)

        with open(output_path, 'wb') as f:
            pickle.dump(existing_results, f)

        torch.save(self.agent.q_function,
                   self.checkpoint_path.format(self.experiment_id,
                                               self.n_checkpoints))
        self.n_checkpoints += 1

    def evaluate_agent(self):
        self.evaluate()
        while True:
            try:
                self.agent.learn_from_environment(self)
            except EndOfLearning:
                print('Learning budget ended. Doing last learning round (if agent wants to)')
                self.agent.learn_from_experience()
                print('Running final evaluation...')
                self.evaluate()
                break
            except Exception as e:
                traceback.print_exc(e)
                print('Ignoring exception and continuing...')

    def print_progress(self):
        print('{} steps ({:.3}%, ETA: {}), {} total reward, explored {} problems. {}'
              .format(self.n_steps,
                      100 * (self.n_steps / self.max_steps),
                      util.format_eta(datetime.datetime.now() - self.begin_time,
                                      self.n_steps,
                                      self.max_steps),
                      self.cumulative_reward,
                      self.n_new_problems,
                      self.agent.stats()))


def evaluate_policy(config, device):
    if config.get('random_policy'):
        q = RandomQFunction()
    elif config.get('inverse_length'):
        q = InverseLength()
    else:
        q = torch.load(config['model_path'], map_location=device)

    q.to(device)
    q.device = device

    env = Environment.from_config(config)
    evaluator = SuccessRatePolicyEvaluator(env, config.get('eval_config', {}))
    result = evaluator.evaluate(q, verbose=True)

    print('Success rate:', result['success_rate'])
    print('Max solution length:', result['max_solution_length'])
    print('Solved problems:', result['successes'])
    print('Unsolved problems:', result['failures'])


def evaluate_policy_checkpoints(config, device):
    previous_successes = set()
    checkpoint_path = config['checkpoint_path']
    env = Environment.from_config(config)
    evaluator = SuccessRatePolicyEvaluator(env, config.get('eval_config', {}))
    i = 0
    last_hash = None

    try:
        while True:
            path = checkpoint_path.format(i)
            i += 1

            with open(path, 'rb') as f:
                h = hashlib.md5(f.read()).hexdigest()
                if h == last_hash:
                    continue
                last_hash = h
            print('Evaluating', path)
            q = torch.load(path, map_location=device)
            q.to(device)
            q.device = device
            result = evaluator.evaluate(q, show_progress=True)

            for j, p in result['successes']:
                if j not in previous_successes:
                    print(f'New success: {j} :: {p.facts[-1]} (length: {result["solution_lengths"][j]})')

            for j, p in result['failures']:
                if j in previous_successes or i == 1:
                    print('New failure:', j, '::', p.facts[-1])

            previous_successes = set([j for j, _ in result['successes']])

            print('Success rate:', result['success_rate'])

    except FileNotFoundError:
        print('Checkpoint', i, 'does not exist -- stopping.')
